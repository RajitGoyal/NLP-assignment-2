{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "import zipfile\n",
        "import gensim.downloader as api\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/Colab Notebooks/Tweets.csv.zip'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    print(\"Contents of ZIP:\", zip_ref.namelist())  # Check inside ZIP\n",
        "    zip_ref.extractall('/content/')  # Extracts to /content/\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/Tweets.csv')[['airline_sentiment', 'text']]\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_tweet(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
        "    text = re.sub(r\"@[\\w]+\", '', text)\n",
        "    text = re.sub(r\"#[\\w]+\", '', text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    contractions = {\"don't\": \"do not\", \"can't\": \"cannot\", \"i'm\": \"i am\", \"it's\": \"it is\"}\n",
        "    for k, v in contractions.items():\n",
        "        text = text.replace(k, v)\n",
        "\n",
        "    tokens = text.split()\n",
        "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words and w.isalpha()]\n",
        "    return tokens\n",
        "\n",
        "df['tokens'] = df['text'].apply(preprocess_tweet)\n",
        "\n",
        "\n",
        "w2v = api.load(\"glove-wiki-gigaword-100\")  # 100D embedding\n",
        "\n",
        "\n",
        "def get_vector(tokens):\n",
        "    vectors = [w2v[word] for word in tokens if word in w2v]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(100)\n",
        "\n",
        "df['vector'] = df['tokens'].apply(get_vector)\n",
        "\n",
        "\n",
        "X = np.vstack(df['vector'].values)\n",
        "y = df['airline_sentiment']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=1000, multi_class='multinomial')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"✅ Model Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "\n",
        "def predict_tweet_sentiment(model, w2v_model, tweet):\n",
        "    tokens = preprocess_tweet(tweet)\n",
        "    vector = get_vector(tokens).reshape(1, -1)\n",
        "    return model.predict(vector)[0]\n",
        "\n",
        "\n",
        "example = \"Flight was delayed but staff was very helpful!\"\n",
        "print(\"Prediction:\", predict_tweet_sentiment(model, w2v, example))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzonC8j7cg6C",
        "outputId": "968eff17-1591-4353-a5de-050e89f480fb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Contents of ZIP: ['Tweets.csv']\n",
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model Accuracy: 0.7568306010928961\n",
            "Prediction: negative\n"
          ]
        }
      ]
    }
  ]
}